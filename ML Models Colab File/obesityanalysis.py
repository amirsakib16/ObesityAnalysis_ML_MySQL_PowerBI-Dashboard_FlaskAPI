# -*- coding: utf-8 -*-
"""ObesityAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iwQ-kQr3fhD3EKk3A0itN4rMf2QG1c-V

# **Import Libraries**
"""

import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OrdinalEncoder
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, f_classif
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SelectKBest
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc

"""# **Import Dataset**"""

dataset = pd.read_csv("/content/Updated_Obesity_Dataset.csv")
dataset.head(10)

"""# **Dataset Summary**"""

dataset.info()

"""# **Dimension of the Dataset**"""

dataset.shape

"""# **Fetch all the Output Categories**"""

dataset["NObeyesdad"].unique()

"""# **Unique Values of Each Column**"""

dataset.nunique()

"""# **Fetching Object Type Columns**"""

dataset.select_dtypes(include="object").columns

"""# **Fetching Numeric Type Columns**"""

dataset.select_dtypes(include="float64").columns

"""# **Splitting Categorical & Numerical Columns**"""

categorical_column = ["Gender","CALC","FAVC",
                      "SCC","SMOKE","family_history_with_overweight",
                      "CAEC","MATRNS","NObeyesdad"]

numerical_column = ["Age","Height","Weight",
                    "FCVC","NCP","CH2O","FAF","TUE"]

"""# **COUNT | MEAN | Standard deviation | MIN | MAX | SPLITTING DATA IN 3 PARTS**"""

numerical_DATA = dataset.select_dtypes(include='number')
numerical_DATA.describe().T

"""# **Varience of The Data**"""

numerical_DATA.var()

"""# **Skew in Numerical Features**"""

numerical_DATA.skew()

"""# **Visual Representation of The Data via Histogram**"""

fig, ax = plt.subplots(figsize=(19, 8), facecolor='beige')
numerical_DATA.hist(bins=50, ax=ax)
ax.set_facecolor('green')
plt.show()

"""# **Density Graph**"""

numerical_DATA = dataset.select_dtypes(include= 'number')
numerical_DATA.plot(kind='density',figsize=(19,14),subplots=True,layout=(6,2),title="Density plot of Numerical features", sharex = False)
plt.show()

"""# **Box Plot**"""

box = dataset.select_dtypes(include=['int64', 'float64']).columns
plt.figure(figsize=(20, 20))
for i, col in enumerate(box, 1):
    plt.subplot(len(box), 1, i)
    sns.boxplot(x=dataset[col], color='lightgreen')
    plt.title(f'Boxplot of {col}', fontsize=12)
    plt.tight_layout()

plt.show()

"""# **Data Distribution of Output Column**"""

class_counts = dataset['NObeyesdad'].value_counts()
print(class_counts)

!pip install imbalanced-learn

class_counts = dataset['NObeyesdad'].value_counts()

plt.figure(figsize=(10, 6))
class_counts.plot(kind='bar', color='red', edgecolor='black')
plt.title('Class Distribution in Output Feature (NObeyesdad)')
plt.xlabel('Class Labels')
plt.ylabel('Number of Instances')
plt.xticks(rotation=45)
plt.tight_layout()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""# **COUNT | UNIQUE | TOP | FREQUENCY**"""

categorical_DATA = dataset.select_dtypes(include = 'object')
categorical_DATA.describe().T

"""# **Total NULL Values of Each Columns**"""

dataset.isnull().sum()

NULL = dataset.isnull().sum().sum()
print(f"Total NULL Values: {NULL}")

"""# **Replace The NULL Values With The Highest Frequency Data For Object Datatype**"""

for i in dataset.select_dtypes(include="object").columns:
    dataset[i].fillna(dataset[i].mode()[0], inplace=True)

"""# **Replace The NULL Values With The Mean Value For Numeric Datatype**"""

simpleimputer = SimpleImputer(strategy="mean")
dataset[numerical_column] = simpleimputer.fit_transform(dataset[numerical_column])
dataset.to_csv("dataset.csv", index=False)

NULL = dataset.isnull().sum().sum()
print(f"Total NULL Values: {NULL}")

"""# **Detect Outliers**"""

numeric_df = dataset.select_dtypes(include='number')
melted_df = numeric_df.melt(var_name='Variable', value_name='Value')

plt.figure(figsize=(20, 10))
sns.boxplot(x='Variable', y='Value', data=melted_df)
plt.xticks(rotation=45)
plt.title("Raw Outlier Data")
plt.tight_layout()
plt.show()
print(f"Total Rows: {dataset.shape[0]}")
print(f'Total Columns: {dataset.shape[-1]}')

outlierColumn = []
for i in dataset.select_dtypes(include="number").columns:
    outlierColumn.append(i)
for i in outlierColumn:
    q1 = dataset[i].quantile(0.25)
    q3 = dataset[i].quantile(0.75)
    IQR = q3-q1
    minRange = q1-(1.5*IQR)
    maxRange = q3+(1.5*IQR)
    dataset = dataset[dataset[i]<=maxRange]
    dataset = dataset[dataset[i]>=minRange]
numeric_df = dataset.select_dtypes(include='number')
melted_df = numeric_df.melt(var_name='Variable', value_name='Value')
plt.figure(figsize=(20, 10))
sns.boxplot(x='Variable', y='Value', data=melted_df)
plt.xticks(rotation=45)
plt.title("After Removing Outlier")
plt.tight_layout()
plt.show()
print(f"Total Rows: {dataset.shape[0]}")
print(f'Total Columns: {dataset.shape[-1]}')

"""# **All Unique Columns' Data**"""

categorical_column = ["Gender","CALC","FAVC",
                      "SCC","SMOKE","family_history_with_overweight",
                      "CAEC","MTRANS","NObeyesdad"]
UNIQUE = {}
for i in categorical_column:
    temp = dataset[i].unique()
    UNIQUE[i] = temp
for i in UNIQUE:
    print(f"{i}:\n   {UNIQUE[i]}")

dataset.to_csv("cleanData.csv", index=False)

"""# **Applying OneHot Encoding**"""

encoder = OneHotEncoder(sparse_output=False)

Gender_encoded = encoder.fit_transform(dataset[['Gender']])
Gender_encoded_df = pd.DataFrame(Gender_encoded, columns=encoder.get_feature_names_out(['Gender']))
Gender_encoded_df.index = dataset.index
dataset = dataset.drop('Gender', axis=1)
dataset = pd.concat([Gender_encoded_df, dataset], axis=1)
dataset = dataset.drop('Gender_Male', axis=1)

FAVC_encoded = encoder.fit_transform(dataset[['FAVC']])
FAVC_encoded_df = pd.DataFrame(FAVC_encoded, columns=encoder.get_feature_names_out(['FAVC']))
FAVC_encoded_df.index = dataset.index
dataset = dataset.drop('FAVC', axis=1)
dataset = pd.concat([FAVC_encoded_df, dataset], axis=1)
dataset = dataset.drop('FAVC_no', axis=1)

SCC_encoded = encoder.fit_transform(dataset[['SCC']])
SCC_encoded_df = pd.DataFrame(SCC_encoded, columns=encoder.get_feature_names_out(['SCC']))
SCC_encoded_df.index = dataset.index
dataset = dataset.drop('SCC', axis=1)
dataset = pd.concat([SCC_encoded_df, dataset], axis=1)
dataset = dataset.drop('SCC_no', axis=1)

SMOKE_encoded = encoder.fit_transform(dataset[['SMOKE']])
SMOKE_encoded_df = pd.DataFrame(SMOKE_encoded, columns=encoder.get_feature_names_out(['SMOKE']))
SMOKE_encoded_df.index = dataset.index
dataset = dataset.drop('SMOKE', axis=1)
dataset = pd.concat([SMOKE_encoded_df, dataset], axis=1)
dataset = dataset.drop('SMOKE_no', axis=1)

family_history_with_overweight_encoded = encoder.fit_transform(dataset[['family_history_with_overweight']])
family_history_with_overweight_encoded_df = pd.DataFrame(family_history_with_overweight_encoded, columns=encoder.get_feature_names_out(['family_history_with_overweight']))
family_history_with_overweight_encoded_df.index = dataset.index
dataset = dataset.drop('family_history_with_overweight', axis=1)
dataset = pd.concat([family_history_with_overweight_encoded_df, dataset], axis=1)
dataset = dataset.drop('family_history_with_overweight_no', axis=1)

"""# **Applying Ordinal Encoding**"""

calcDATA = [['no', 'Sometimes', 'Frequently', 'Always']]
ordinalencoder = OrdinalEncoder(categories=calcDATA)
result = ordinalencoder.fit_transform(dataset[["CALC"]])
dataset = dataset.drop('CALC', axis=1)
dataset.insert(loc=8, column="CALC", value=result)


caecDATA = [['no', 'Sometimes', 'Frequently', 'Always']]
ordinalencoder = OrdinalEncoder(categories=caecDATA)
result = ordinalencoder.fit_transform(dataset[["CAEC"]])
dataset = dataset.drop('CAEC', axis=1)
dataset.insert(loc=8, column="CAEC", value=result)


mtransDATA = [['Public_Transportation', 'Walking', 'Automobile', 'Motorbike', 'Bike']]
ordinalencoder = OrdinalEncoder(categories=mtransDATA)
result = ordinalencoder.fit_transform(dataset[["MTRANS"]])
dataset = dataset.drop('MTRANS', axis=1)
dataset.insert(loc=8, column="MTRANS", value=result)


nobeyesdadDATA = [['Normal_Weight', 'Overweight_Level_I', 'Overweight_Level_II',
                'Obesity_Type_I', 'Insufficient_Weight', 'Obesity_Type_II', 'Obesity_Type_III']]
ordinalencoder = OrdinalEncoder(categories=nobeyesdadDATA)
result = ordinalencoder.fit_transform(dataset[["NObeyesdad"]])
dataset = dataset.drop('NObeyesdad', axis=1)
dataset.insert(loc=16, column="NObeyesdad", value=result)

dataset.head(10)

"""# **Find The Total Number of Duplicated Rows and Columns**"""

print(f"Duplicated Rows: {dataset.duplicated().sum()}")
print(f'Duplicated Columns: {dataset.T.duplicated().sum()}')
print(f"Total Rows: {dataset.shape[0]}")
print(f'Total Columns: {dataset.shape[-1]}')

"""# **All the Duplicate Columns**"""

dataset[dataset.duplicated(keep=False)]

"""# **Remove Duplicate Rows**"""

dataset = dataset.drop_duplicates()
dataset.to_csv('dataset.csv', index=False)
print(f"Total Rows: {dataset.shape[0]}")
print(f'Total Columns: {dataset.shape[-1]}')

"""# **Correlation Matrix**"""

dataset.corr()

"""# **Visual Representation of Correlation via HeatMap**
# **Full Dataset**
"""

CM = dataset.corr()
plt.figure(figsize=(18, 6))
sns.heatmap(CM, annot=True, cmap='vanimo', fmt='.3f', linewidths=0.)
plt.savefig('correlation_heatmap.png', dpi=300, bbox_inches='tight')
plt.show()

"""# **Train Test Split with Train set (70%) | Test set (30%)**"""

input = dataset.drop("NObeyesdad", axis=1)
output = dataset["NObeyesdad"]
X_train, X_test, y_train, y_test  = train_test_split(input, output, test_size=0.3, random_state=42)

"""# **Data Shape**"""

X_train.shape, X_test.shape

"""# **X_train**"""

CMtrx = X_train.corr()
plt.figure(figsize=(18, 6))
sns.heatmap(CMtrx, annot=True, cmap='RdGy', fmt='.3f', linewidths=0.)
plt.show()

"""# **Feature Selection**

# **Correlation and Drop Columns**
"""

corr_Matrix = X_train.corr()
columns = corr_Matrix.columns
columns_to_Drop = []
for i in range(len(columns)):
    for j in range(i+1, len(columns)):
        if corr_Matrix.loc[columns[i], columns[j]] > 0.85:
            columns_to_Drop.append(columns[j])
print(len(columns_to_Drop))

"""# **Varience Threshold**"""

VT = VarianceThreshold(threshold=0.05)
VT.fit(X_train)

print(sum(VT.get_support()))

columns = X_train.columns[VT.get_support()]
X_train = VT.transform(X_train)
X_test = VT.transform(X_test)

X_train = pd.DataFrame(X_train, columns=columns)
X_test = pd.DataFrame(X_test, columns=columns)

"""# **Shape of Training and Test Data**"""

X_train.shape, X_test.shape

"""# **X_train DataFrame**"""

X_train.head(100)

X_test.head(100)



categorical_data = ["Gender","CALC","FAVC",
                      "SCC","SMOKE","family_history_with_overweight",
                      "CAEC","MTRANS","NObeyesdad"]
print(len(categorical_data))

"""# **Standardization**"""

to_scale = ['Age', 'Weight']


scaler = StandardScaler()
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[to_scale] = scaler.fit_transform(X_train[to_scale])
X_test_scaled[to_scale] = scaler.transform(X_test[to_scale])

from imblearn.over_sampling import SMOTE

# Apply SMOTE on the scaled training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

"""# **MODELS**

# **Decision Tree**
"""

DT = DecisionTreeClassifier(random_state=20)
DT.fit(X_train_resampled, y_train_resampled)  # Use resampled balanced data
y_pred_dt = DT.predict(X_test_scaled)         # Predict on scaled test set

accuracyDT = accuracy_score(y_test, y_pred_dt) * 100
precisionDT = precision_score(y_test, y_pred_dt, average='weighted') * 100
recallDT = recall_score(y_test, y_pred_dt, average='weighted') * 100
f1DT = f1_score(y_test, y_pred_dt, average='weighted') * 100

print(f"Accuracy Score: {accuracyDT:.4f}")
print(f"Precision Score: {precisionDT:.4f}")
print(f"Recall Score: {recallDT:.4f}")
print(f"F1 Score: {f1DT:.4f}")

correct = np.sum(np.array(y_test) == np.array(y_pred_dt))
total = len(y_test)
wrong = total - correct

print(f"Correct Predictions: {correct}")
print(f"Wrong Predictions: {wrong}")

"""# **Random Forest Classifier with Scaled Data**"""

rf_model = RandomForestClassifier(random_state=19)
rf_model.fit(X_train_resampled, y_train_resampled)
y_pred = rf_model.predict(X_test_scaled)

y_pred_rf = rf_model.predict(X_test_scaled)
y_prob_rf = rf_model.predict_proba(X_test_scaled)

accuracyRF = accuracy_score(y_test, y_pred) * 100
PrecisionRF = precision_score(y_test, y_pred_rf, average="macro")*100
RecallRF = recall_score(y_test, y_pred_rf, average="macro")*100
F1RF = f1_score(y_test, y_pred_rf, average="macro")*100

print(f'Random Forest accuracy: {accuracyRF:.2f}%')
print(f'Precision Score (macro): {PrecisionRF:.2f}')
print(f'Recall Score (macro): {RecallRF:.2f}')
print(f'F1 Score (macro): {F1RF:.2f}')


correct = np.sum(np.array(y_test) == np.array(y_pred_rf))
total = len(y_test)
wrong = total - correct

print(f"Correct Predictions: {correct}")
print(f"Wrong Predictions: {wrong}")

import joblib
joblib.dump(rf_model, 'random_forest_model.pkl')
from google.colab import files
files.download('random_forest_model.pkl')

rf_model = RandomForestClassifier(random_state=18)
rf_model.fit(X_train_resampled, y_train_resampled)
y_pred = rf_model.predict(X_test)

y_pred_rf = rf_model.predict(X_test)
y_prob_rf = rf_model.predict_proba(X_test)

X_accuracyRF = accuracy_score(y_test, y_pred) * 100
X_PrecisionRF = precision_score(y_test, y_pred_rf, average="macro")*100
X_RecallRF = recall_score(y_test, y_pred_rf, average="macro")*100
X_F1RF = f1_score(y_test, y_pred_rf, average="macro")*100

print(f'Random Forest accuracy: {X_accuracyRF:.2f}%')
print(f'Precision Score (macro): {X_PrecisionRF:.2f}')
print(f'Recall Score (macro): {X_RecallRF:.2f}')
print(f'F1 Score (macro): {X_F1RF:.2f}')

correct = np.sum(np.array(y_test) == np.array(y_pred_rf))
total = len(y_test)
wrong = total - correct

print(f"Correct Predictions: {correct}")
print(f"Wrong Predictions: {wrong}")

"""# **Logistic Regression with Scaled Data**"""

log_reg = LogisticRegression(max_iter=100)
log_reg.fit(X_train_resampled, y_train_resampled)

y_pred_log = log_reg.predict(X_test_scaled)
y_prob_log = log_reg.predict_proba(X_test_scaled)[:, 1]

accuracyLGR = accuracy_score(y_test, y_pred_log) * 100
PrecisionLGR = precision_score(y_test, y_pred_log, average="macro")* 100
RecallLGR = recall_score(y_test, y_pred_log, average="macro")* 100
F1LGR = f1_score(y_test, y_pred_log, average="macro")* 100

print(f'Accuracy (Logistic Regression): {accuracyLGR}%')
print(f'Precision Score (macro): {PrecisionLGR:.2f}')
print(f'Recall Score (macro): {RecallLGR:.2f}')
print(f'F1 Score (macro): {F1LGR:.2f}')

correct = np.sum(np.array(y_test) == np.array(y_pred_log))
total = len(y_test)
wrong = total - correct

print(f"Correct Predictions: {correct}")
print(f"Wrong Predictions: {wrong}")

log_reg = LogisticRegression(max_iter=100)
log_reg.fit(X_train_resampled, y_train_resampled)

y_pred_log = log_reg.predict(X_test)
y_prob_log = log_reg.predict_proba(X_test)[:, 1]

X_accuracyLGR = accuracy_score(y_test, y_pred_log) * 100
X_PrecisionLGR = precision_score(y_test, y_pred_log, average="macro")* 100
X_RecallLGR = recall_score(y_test, y_pred_log, average="macro")* 100
X_F1LGR = f1_score(y_test, y_pred_log, average="macro")* 100

print(f'Accuracy (Logistic Regression): {X_accuracyLGR}%')
print(f'Precision Score (macro): {X_PrecisionLGR:.2f}')
print(f'Recall Score (macro): {X_RecallLGR:.2f}')
print(f'F1 Score (macro): {X_F1LGR:.2f}')

correct = np.sum(np.array(y_test) == np.array(y_pred_log))
total = len(y_test)
wrong = total - correct

print(f"Correct Predictions: {correct}")
print(f"Wrong Predictions: {wrong}")

"""# **Neural Network with Scaled Data**"""

mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=800, random_state=42)
mlp.fit(X_train_resampled, y_train_resampled)

y_pred = mlp.predict(X_test_scaled)

accuracyNN = accuracy_score(y_test, y_pred) * 100
precisionNN = precision_score(y_test, y_pred, average='weighted') * 100
recallNN = recall_score(y_test, y_pred, average='weighted') * 100
f1NN = f1_score(y_test, y_pred, average='weighted') * 100

print(f"Accuracy: {accuracyNN:.2f}%")
print(f"Precision: {precisionNN:.2f}%")
print(f"Recall: {recallNN:.2f}%")
print(f"F1 Score: {f1NN:.2f}%")

correct = np.sum(np.array(y_test) == np.array(y_pred))
total = len(y_test)
wrong = total - correct

print(f"Correct Predictions: {correct}")
print(f"Wrong Predictions: {wrong}")

mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=800, random_state=42)
mlp.fit(X_train_resampled, y_train_resampled)

y_pred = mlp.predict(X_test)

X_accuracyNN = accuracy_score(y_test, y_pred) * 100
X_precisionNN = precision_score(y_test, y_pred, average='weighted') * 100
X_recallNN = recall_score(y_test, y_pred, average='weighted') * 100
X_f1NN = f1_score(y_test, y_pred, average='weighted') * 100

print(f"Accuracy: {X_accuracyNN:.2f}%")
print(f"Precision: {X_precisionNN:.2f}%")
print(f"Recall: {X_recallNN:.2f}%")
print(f"F1 Score: {X_f1NN:.2f}%")


correct = np.sum(np.array(y_test) == np.array(y_pred))
total = len(y_test)
wrong = total - correct

print(f"Correct Predictions: {correct}")
print(f"Wrong Predictions: {wrong}")

"""# **DASHBOARD**

# **Comparison among Accuracy | Precision | Recall | F1 Score**
"""

import numpy as np
import matplotlib.pyplot as plt

# Define your metrics
models_all = [ 'Random Forest', 'Logistic Regression', 'Neural Network', 'Decision Tree']
accuracies = [accuracyRF, accuracyLGR, accuracyNN, accuracyDT]

models_precision = [ 'Random Forest', 'Logistic Regression', 'Neural Network', 'Decision Tree']
Precision = [PrecisionRF, PrecisionLGR, precisionNN, precisionDT]

models_recall = [ 'Random Forest', 'Logistic Regression', 'Neural Network', 'Decision Tree']
recall = [RecallRF, RecallLGR, recallNN, recallDT]

models_f1 = [ 'Random Forest', 'Logistic Regression', 'Neural Network', 'Decision Tree']
f1_scores = [F1RF, F1LGR, f1NN, f1DT]

# Sort by accuracy
sorted_indices = np.argsort(accuracies)[::-1]
models_sorted = [models_all[i] for i in sorted_indices]
accuracies_sorted = [accuracies[i] for i in sorted_indices]
colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(accuracies_sorted)))

# Create subplots
fig, axs = plt.subplots(2, 2, figsize=(16, 9))
ax1, ax2, ax3, ax4 = axs[0, 0], axs[0, 1], axs[1, 0], axs[1, 1]

# Accuracy Plot
bars1 = ax1.bar(models_sorted, accuracies_sorted, color=colors, edgecolor='black')
ax1.plot(models_sorted, accuracies_sorted, marker='o', linestyle='-', color='black', linewidth=1)
for bar, acc in zip(bars1, accuracies_sorted):
    yval = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width()/2.0, yval + 1, f'{acc:.2f}%',
             ha='center', va='bottom', fontsize=12, fontweight='bold', color='green')
ax1.set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
ax1.set_ylabel('Accuracy (%)', fontsize=12)
ax1.set_ylim(0, 110)
ax1.grid(axis='y', linestyle='--', alpha=0.7)
ax1.tick_params(axis='x', rotation=15)

# Precision Plot
bars2 = ax2.barh(models_precision, Precision, color='lightgreen', edgecolor='black')
for bar, prec in zip(bars2, Precision):
    ax2.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
             f'{prec:.2f}%', va='center', fontsize=12)
ax2.set_title('Model Precision Comparison', fontsize=14, fontweight='bold')
ax2.set_xlabel('Precision (%)', fontsize=12)
ax2.set_xlim(0, 110)
ax2.grid(axis='x', linestyle='--', alpha=0.7)

# Recall Plot
bars3 = ax3.barh(models_recall, recall, color='skyblue', edgecolor='black')
for bar, rec in zip(bars3, recall):
    ax3.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,
             f'{rec:.2f}%', va='center', fontsize=12)
ax3.set_title('Model Recall Comparison', fontsize=14, fontweight='bold')
ax3.set_xlabel('Recall (%)', fontsize=12)
ax3.set_xlim(0, 110)
ax3.grid(axis='x', linestyle='--', alpha=0.7)

# F1 Score Plot
bars4 = ax4.bar(models_f1, f1_scores, color='orange', edgecolor='black')
for bar, score in zip(bars4, f1_scores):
    yval = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width()/2.0, yval + 1, f'{score:.2f}%',
             ha='center', va='bottom', fontsize=12)
ax4.set_title('F1 Score Comparison', fontsize=14, fontweight='bold')
ax4.set_ylabel('F1 Score (%)', fontsize=12)
ax4.set_ylim(0, 110)
ax4.grid(axis='y', linestyle='--', alpha=0.7)
ax4.tick_params(axis='x', rotation=15)

plt.tight_layout()
plt.show()

"""# **Comparison Between Scaled and Unscaled Model**"""

models = ['Random Forest', 'Logistic Regression', 'Neural Network']

acc = [accuracyRF, accuracyLGR, accuracyNN]
X_acc = [X_accuracyRF, X_accuracyLGR, X_accuracyNN]


prec = [PrecisionRF, PrecisionLGR, precisionNN]
X_prec = [X_PrecisionRF, X_PrecisionLGR, X_precisionNN]


rec = [RecallRF, RecallLGR, recallNN]
X_rec = [X_RecallRF, X_RecallLGR, X_recallNN]


f1s = [F1RF, F1LGR, f1NN]
X_f1s = [X_F1RF, X_F1LGR, X_f1NN]


x = np.arange(len(models))
width = 0.35

fig, axs = plt.subplots(2, 2, figsize=(16, 8))

def annotate_bars(ax, bars):
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.1f}%',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=8)

bars1 = axs[0, 0].bar(x - width/2, acc, width, label='Scaled', color='cornflowerblue', edgecolor='black')
bars2 = axs[0, 0].bar(x + width/2, X_acc, width, label='Not Scaled', color='orange', edgecolor='black')
axs[0, 0].set_title('Accuracy Comparison')
axs[0, 0].set_xticks(x)
axs[0, 0].set_xticklabels(models, rotation=15)
axs[0, 0].set_ylabel('Accuracy (%)')
axs[0, 0].legend()
axs[0, 0].grid(axis='y', linestyle='--', alpha=0.7)
annotate_bars(axs[0, 0], bars1)
annotate_bars(axs[0, 0], bars2)

bars3 = axs[0, 1].bar(x - width/2, prec, width, label='Scaled', color='lightgreen', edgecolor='black')
bars4 = axs[0, 1].bar(x + width/2, X_prec, width, label='Not Scaled', color='tomato', edgecolor='black')
axs[0, 1].set_title('Precision Comparison')
axs[0, 1].set_xticks(x)
axs[0, 1].set_xticklabels(models, rotation=15)
axs[0, 1].set_ylabel('Precision (%)')
axs[0, 1].legend()
axs[0, 1].grid(axis='y', linestyle='--', alpha=0.7)
annotate_bars(axs[0, 1], bars3)
annotate_bars(axs[0, 1], bars4)

bars5 = axs[1, 0].bar(x - width/2, rec, width, label='Scaled', color='skyblue', edgecolor='black')
bars6 = axs[1, 0].bar(x + width/2, X_rec, width, label='Not Scaled', color='gold', edgecolor='black')
axs[1, 0].set_title('Recall Comparison')
axs[1, 0].set_xticks(x)
axs[1, 0].set_xticklabels(models, rotation=15)
axs[1, 0].set_ylabel('Recall (%)')
axs[1, 0].legend()
axs[1, 0].grid(axis='y', linestyle='--', alpha=0.7)
annotate_bars(axs[1, 0], bars5)
annotate_bars(axs[1, 0], bars6)

bars7 = axs[1, 1].bar(x - width/2, f1s, width, label='Scaled', color='mediumpurple', edgecolor='black')
bars8 = axs[1, 1].bar(x + width/2, X_f1s, width, label='Not Scaled', color='crimson', edgecolor='black')
axs[1, 1].set_title('F1 Score Comparison')
axs[1, 1].set_xticks(x)
axs[1, 1].set_xticklabels(models, rotation=15)
axs[1, 1].set_ylabel('F1 Score (%)')
axs[1, 1].legend()
axs[1, 1].grid(axis='y', linestyle='--', alpha=0.7)
annotate_bars(axs[1, 1], bars7)
annotate_bars(axs[1, 1], bars8)

plt.tight_layout()
plt.show()

"""# **Confusion Matrix Display**"""

models = [
    ('Decision Tree', DT),
    ('Random Forest', rf_model),
    ('Logistic Regression', log_reg),
    ('Neural Network', mlp)
]

fig, axs = plt.subplots(1, len(models), figsize=(22, 6))

for i, (name, model) in enumerate(models):
    y_pred = model.predict(X_test_scaled)

    cm = confusion_matrix(y_test, y_pred)
    acc = accuracy_score(y_test, y_pred)

    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(ax=axs[i], colorbar=False)
    axs[i].set_title(f'{name}\nAccuracy: {acc:.2%}')

plt.suptitle("Confusion Matrices", fontsize=18)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

"""# **ROC Curve**

# **Random Forest ROC Curve**
"""

classes = np.unique(y_test)
n_classes = len(classes)

y_test_bin = label_binarize(y_test, classes=classes)
y_score_scaled = rf_model.predict_proba(X_test_scaled)
y_score_unscaled = rf_model.predict_proba(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

for i in range(n_classes):
    fpr_s, tpr_s, _ = roc_curve(y_test_bin[:, i], y_score_scaled[:, i])
    auc_s = auc(fpr_s, tpr_s)
    ax1.plot(fpr_s, tpr_s, label=f'Class {classes[i]} (AUC = {auc_s:.2f})')

ax1.plot([0, 1], [0, 1], 'k--', label='Random Forest')
ax1.set_title('Random Forest - ROC Curve (Scaled Data)')
ax1.set_xlabel('False Positive Rate')
ax1.set_ylabel('True Positive Rate')
ax1.legend(loc='lower right')
ax1.grid(True)

for i in range(n_classes):
    fpr_u, tpr_u, _ = roc_curve(y_test_bin[:, i], y_score_unscaled[:, i])
    auc_u = auc(fpr_u, tpr_u)
    ax2.plot(fpr_u, tpr_u, label=f'Class {classes[i]} (AUC = {auc_u:.2f})')

ax2.plot([0, 1], [0, 1], 'k--', label='Random Forest')
ax2.set_title('Random Forest - ROC Curve (Unscaled Data)')
ax2.set_xlabel('False Positive Rate')
ax2.set_ylabel('True Positive Rate')
ax2.legend(loc='lower right')
ax2.grid(True)

plt.tight_layout()
plt.show()

"""# **Logistic Regression ROC Curve**"""

classes = np.unique(y_test)
n_classes = len(classes)

y_test_bin = label_binarize(y_test, classes=classes)
y_score_scaled = log_reg.predict_proba(X_test_scaled)
y_score_unscaled = log_reg.predict_proba(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

for i in range(n_classes):
    fpr_s, tpr_s, _ = roc_curve(y_test_bin[:, i], y_score_scaled[:, i])
    auc_s = auc(fpr_s, tpr_s)
    ax1.plot(fpr_s, tpr_s, label=f'Class {classes[i]} (AUC = {auc_s:.2f})')

ax1.plot([0, 1], [0, 1], 'k--', label='Logistic Regression')
ax1.set_title('Logistic Regression - ROC Curve (Scaled Data)')
ax1.set_xlabel('False Positive Rate')
ax1.set_ylabel('True Positive Rate')
ax1.legend(loc='lower right')
ax1.grid(True)

for i in range(n_classes):
    fpr_u, tpr_u, _ = roc_curve(y_test_bin[:, i], y_score_unscaled[:, i])
    auc_u = auc(fpr_u, tpr_u)
    ax2.plot(fpr_u, tpr_u, label=f'Class {classes[i]} (AUC = {auc_u:.2f})')

ax2.plot([0, 1], [0, 1], 'k--', label='Logistic Regression')
ax2.set_title('Logistic Regression - ROC Curve (Unscaled Data)')
ax2.set_xlabel('False Positive Rate')
ax2.set_ylabel('True Positive Rate')
ax2.legend(loc='lower right')
ax2.grid(True)

plt.tight_layout()
plt.show()

"""# **Neural Network ROC Curve**"""

classes = np.unique(y_test)
n_classes = len(classes)

y_test_bin = label_binarize(y_test, classes=classes)
y_score_scaled = mlp.predict_proba(X_test_scaled)
y_score_unscaled = mlp.predict_proba(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

for i in range(n_classes):
    fpr_s, tpr_s, _ = roc_curve(y_test_bin[:, i], y_score_scaled[:, i])
    auc_s = auc(fpr_s, tpr_s)
    ax1.plot(fpr_s, tpr_s, label=f'Class {classes[i]} (AUC = {auc_s:.2f})')

ax1.plot([0, 1], [0, 1], 'k--', label='Neural Network')
ax1.set_title('Neural Network - ROC Curve (Scaled Data)')
ax1.set_xlabel('False Positive Rate')
ax1.set_ylabel('True Positive Rate')
ax1.legend(loc='lower right')
ax1.grid(True)

for i in range(n_classes):
    fpr_u, tpr_u, _ = roc_curve(y_test_bin[:, i], y_score_unscaled[:, i])
    auc_u = auc(fpr_u, tpr_u)
    ax2.plot(fpr_u, tpr_u, label=f'Class {classes[i]} (AUC = {auc_u:.2f})')

ax2.plot([0, 1], [0, 1], 'k--', label='Neural Network')
ax2.set_title('Neural Network - ROC Curve (Unscaled Data)')
ax2.set_xlabel('False Positive Rate')
ax2.set_ylabel('True Positive Rate')
ax2.legend(loc='lower right')
ax2.grid(True)

plt.tight_layout()
plt.show()

"""# **Decision Tree ROC Curve**"""

classes = np.unique(y_test)
n_classes = len(classes)

y_test_bin = label_binarize(y_test, classes=classes)
y_score_scaled = DT.predict_proba(X_test_scaled)
y_score_unscaled = DT.predict_proba(X_test)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

for i in range(n_classes):
    fpr_s, tpr_s, _ = roc_curve(y_test_bin[:, i], y_score_scaled[:, i])
    auc_s = auc(fpr_s, tpr_s)
    ax1.plot(fpr_s, tpr_s, label=f'Class {classes[i]} (AUC = {auc_s:.2f})')

ax1.plot([0, 1], [0, 1], 'k--', label='Decision Tree')
ax1.set_title('Decision Tree - ROC Curve (Scaled Data)')
ax1.set_xlabel('False Positive Rate')
ax1.set_ylabel('True Positive Rate')
ax1.legend(loc='lower right')
ax1.grid(True)

for i in range(n_classes):
    fpr_u, tpr_u, _ = roc_curve(y_test_bin[:, i], y_score_unscaled[:, i])
    auc_u = auc(fpr_u, tpr_u)
    ax2.plot(fpr_u, tpr_u, label=f'Class {classes[i]} (AUC = {auc_u:.2f})')

ax2.plot([0, 1], [0, 1], 'k--', label='Decision Tree')
ax2.set_title('Decision Tree - ROC Curve (Unscaled Data)')
ax2.set_xlabel('False Positive Rate')
ax2.set_ylabel('True Positive Rate')
ax2.legend(loc='lower right')
ax2.grid(True)

plt.tight_layout()
plt.show()

classes = np.unique(y_test)
n_classes = len(classes)
y_test_bin = label_binarize(y_test, classes=classes)


fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))
axes = axes.ravel()

for i, (name, model) in enumerate(models):
    y_score_scaled = model.predict_proba(X_test_scaled)
    y_score_unscaled = model.predict_proba(X_test)

    auc_scores_scaled = []
    auc_scores_unscaled = []

    for j in range(n_classes):
        fpr_s, tpr_s, _ = roc_curve(y_test_bin[:, j], y_score_scaled[:, j])
        fpr_u, tpr_u, _ = roc_curve(y_test_bin[:, j], y_score_unscaled[:, j])

        auc_s = auc(fpr_s, tpr_s)
        auc_u = auc(fpr_u, tpr_u)

        auc_scores_scaled.append(auc_s)
        auc_scores_unscaled.append(auc_u)

    x = np.arange(n_classes)
    width = 0.35
    ax = axes[i]
    bars1 = ax.bar(x - width/2, auc_scores_scaled, width, label='Scaled')
    bars2 = ax.bar(x + width/2, auc_scores_unscaled, width, label='Unscaled')

    ax.set_title(name)
    ax.set_xticks(x)
    ax.set_xticklabels([f'Class {cls}' for cls in classes])
    ax.set_ylim(0.0, 1.1)
    ax.set_ylabel("AUC Score")
    ax.legend()
    ax.grid(True, axis='y')

    # Annotate bars
    for bar in bars1 + bars2:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}', xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3), textcoords="offset points",
                    ha='center', va='bottom')

if len(models) < 6:
    for j in range(len(models), 6):
        fig.delaxes(axes[j])

plt.suptitle('AUC by Class for All Models (Scaled vs Unscaled)', fontsize=16)
plt.tight_layout(rect=[0, 0, 1, 0.96])
plt.show()